import argparse, torch, numpy as np
from torch.utils.data import DataLoader
import torch.nn.functional as F
from pathlib import Path
from tqdm import tqdm
from lerobot_dataset.pick_place_mj_builder import MjPickPlaceOfflineDataset
from models.critic import ValueNet, QNet

NUM_PHASES=6

class TransitionDataset(torch.utils.data.Dataset):
    def __init__(self, base, discount=0.99, shaped=False, progress=False):
        self.base = base
        self.discount = discount
        self.shaped = shaped
        self.progress = progress
    def __len__(self): return len(self.base)
    def __getitem__(self, idx):
        it = self.base[idx]
        s  = it["obs_state"].astype(np.float32)
        ns = it["next_obs_state"].astype(np.float32)
        a  = it["action"].astype(np.float32)
        done  = float(it["done"])
        success = float(it["success"])
        r = success if done and success > 0 else 0.0
        if self.shaped:
            cube = s[4:7]; tgt = s[7:9]
            dist_xy = np.linalg.norm(cube[:2]-tgt)
            r += -0.02 * dist_xy
        if self.progress:
            cube = s[4:7]; tgt = s[7:9]
            n_cube = ns[4:7]
            d_now = np.linalg.norm(cube[:2]-tgt)
            d_next= np.linalg.norm(n_cube[:2]-tgt)
            prog = d_now - d_next
            r += 0.1 * prog
        return {
            "s": s, "a": a, "ns": ns,
            "r": np.float32(r), "d": np.float32(done),
            "phase": it["phase_id"]
        }

def expectile_loss(diff, expectile):
    w = torch.where(diff > 0, expectile, 1-expectile)
    return (w * diff.pow(2)).mean()

def compute_phase_weights(base):
    counts = np.zeros(NUM_PHASES, dtype=np.int64)
    for i in range(len(base)):
        counts[base[i]["phase_id"]] += 1
    counts = np.maximum(counts,1)
    inv = 1.0 / counts
    w = inv / inv.sum() * NUM_PHASES
    return torch.tensor(w, dtype=torch.float32)

def to_tensor(x, device):
    if isinstance(x, torch.Tensor): return x.to(device)
    return torch.as_tensor(x, device=device)

def main(args):
    base = MjPickPlaceOfflineDataset(args.data_root, use_paraphrase=False)
    ds = TransitionDataset(base, discount=args.gamma, shaped=args.shaped_reward, progress=args.progress_reward)
    loader = DataLoader(ds, batch_size=args.batch_size, shuffle=True, drop_last=True)
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

    state_dim = base[0]["obs_state"].shape[0]
    action_dim= base[0]["action"].shape[0]
    vnet = ValueNet(state_dim).to(device)
    qnet = QNet(state_dim, action_dim).to(device)

    out_dir = Path(args.out_dir); out_dir.mkdir(parents=True, exist_ok=True)
    if args.resume and (out_dir/"qnet.pt").exists():
        qnet.load_state_dict(torch.load(out_dir/"qnet.pt", map_location=device))
        vnet.load_state_dict(torch.load(out_dir/"vnet.pt", map_location=device))
        print("Resumed previous critic weights.")

    opt_v = torch.optim.Adam(vnet.parameters(), lr=args.lr)
    opt_q = torch.optim.Adam(qnet.parameters(), lr=args.lr)

    phase_weights = compute_phase_weights(base).to(device) if args.phase_balance else None
    if phase_weights is not None:
        print("Critic phase weights:", phase_weights.cpu().numpy())

    for epoch in range(1, args.epochs+1):
        q_losses=[]; v_losses=[]
        for batch in tqdm(loader, desc=f"epoch {epoch}"):
            s  = to_tensor(batch["s"], device)
            a  = to_tensor(batch["a"], device)
            ns = to_tensor(batch["ns"], device)
            r  = to_tensor(batch["r"], device)
            d  = to_tensor(batch["d"], device)
            p  = torch.as_tensor(batch["phase"], device=device, dtype=torch.long)

            with torch.no_grad():
                v_ns = vnet(ns, p)
                target_q = r + args.gamma * (1 - d) * v_ns

            q_pred = qnet(s,a,p)
            if phase_weights is not None:
                w = phase_weights[p]
                loss_q = ((q_pred - target_q).pow(2) * w).mean()
            else:
                loss_q = F.mse_loss(q_pred, target_q)
            opt_q.zero_grad()
            loss_q.backward()
            if args.grad_clip>0:
                torch.nn.utils.clip_grad_norm_(qnet.parameters(), args.grad_clip)
            opt_q.step()

            with torch.no_grad():
                q_det = qnet(s,a,p)
            diff = q_det - vnet(s,p)
            if phase_weights is not None:
                wv = phase_weights[p]
                loss_v = expectile_loss(diff, args.expectile) * (wv.mean())
            else:
                loss_v = expectile_loss(diff, args.expectile)
            opt_v.zero_grad()
            loss_v.backward()
            if args.grad_clip>0:
                torch.nn.utils.clip_grad_norm_(vnet.parameters(), args.grad_clip)
            opt_v.step()

            q_losses.append(loss_q.item())
            v_losses.append(loss_v.item())

        print(f"epoch {epoch} q_loss={np.mean(q_losses):.5f} v_loss={np.mean(v_losses):.5f}")
        torch.save(qnet.state_dict(), out_dir/"qnet.pt")
        torch.save(vnet.state_dict(), out_dir/"vnet.pt")
        print("Saved critics epoch", epoch)

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_root", type=str, default="data/raw/mj_pick_place_v5")
    ap.add_argument("--batch_size", type=int, default=128)
    ap.add_argument("--lr", type=float, default=3e-4)
    ap.add_argument("--epochs", type=int, default=5)
    ap.add_argument("--gamma", type=float, default=0.99)
    ap.add_argument("--expectile", type=float, default=0.7)
    ap.add_argument("--shaped_reward", action="store_true")
    ap.add_argument("--progress_reward", action="store_true")
    ap.add_argument("--phase_balance", action="store_true")
    ap.add_argument("--grad_clip", type=float, default=1.0)
    ap.add_argument("--resume", action="store_true")
    ap.add_argument("--out_dir", type=str, default="models/ckpts_iql_balanced")
    args = ap.parse_args()
    main(args)
